<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>ForBES</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="inset-text">ForBES</h1>
        <p>Generic and efficient MATLAB solver for nonsmooth convex optimization problems</p>
        <p class="view"><a href="https://github.com/lostella/ForBES">View the Project on GitHub <small>lostella/ForBES</small></a></p>
        <ul>
          <li><a href="https://github.com/lostella/ForBES/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/lostella/ForBES/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/lostella/ForBES">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <p><strong>ForBES</strong> (standing for <strong>For</strong>ward-<strong>B</strong>ackward <strong>E</strong>nvelope <strong>S</strong>olver) is a MATLAB solver for
nonsmooth convex optimization problems.</p>

<p>It is generic in the sense that the user can customize the problem to solve in an easy and flexible way.
It is efficient since it features very efficient algorithms, suited for large scale applications.</p>

<p>Here is a performance comparison between ForBES, the fast forward-backward splitting method (also
known as fast proximal gradient method) and ADMM (alternating direction method of multipliers),
applied to a Lasso problem with 3K observations and 500K features, for a total of 7.5M nonzero coefficients.</p>

<p align="center">
<img src="figures/lasso_random_3e3_5e5_lambda_3e-1_noborder.png" width="640" height="404">
</p>

<h2 class="inset-text">
<a id="installation" class="anchor" href="#installation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Installation</h2>

<p>Simply click on one of the links on top of the page, and either clone the GitHub repository or uncompress the
zip or tar.gz archive. Then move with the MATLAB command line to
the directory of ForBES. Compile all the <em>mex</em>-files required by simply hitting</p>

<pre><code>&gt; make
</code></pre>

<h2 class="inset-text">
<a id="how-to-use-it" class="anchor" href="#how-to-use-it" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use it</h2>

<p>ForBES consists mainly of two MATLAB routines, <code>minfbe</code> and <code>miname</code>.
In order to use them one must provide a description of the problem in a MATLAB
structure and (optionally) a set of options:</p>

<pre><code>out = minfbe(prob, opt);
out = miname(prob, opt);
</code></pre>

<p>Structure <code>prob</code> contains attributes describing the details of the problem, such as objective
terms and constraints, while <code>opt</code> describes, e.g., details on the algorithm to use, termination
criteria, the level of verbosity, and so on. In the following we describe more in detail how to define
these structures. Output <code>out</code> will contain the results of the optimization process.</p>

<p>Examples on how to use <code>minfbe</code> and <code>miname</code> can be found in the <a href="https://github.com/lostella/ForBES/tree/master/tests">tests folder</a>. Furthermore, you can access the help file of the solvers directly from MATLAB with</p>

<pre><code>&gt; help minfbe
&gt; help miname
</code></pre>

<h2 class="inset-text">
<a id="minfbe-convex-composite-problems" class="anchor" href="#minfbe-convex-composite-problems" aria-hidden="true"><span class="octicon octicon-link"></span></a>minfbe (convex composite problems)</h2>

<p>We consider here problems in the form</p>

$$ \mathrm{minimize}\ f_1(Ax-b) + \langle\ell, x\rangle + f_2(Cx-d) + g(x) \tag{1}$$

<p>where $f_1$ is convex quadratic, $\ell$ is a linear term and $f_2$ is any convex, twice continuously
differentiable function with Lipschitz continuous gradient. Any of these terms may be omitted in the
problem definition, in which case it is assumed to be identically zero.
Function $g$ is a general proper, closed, convex function (possibly nonsmooth).
This form of includes many practical problems arising in several fields such as optimal
control, data analysis, machine learning, image and signal processing to name a few.</p>

<p>Since $f_1$ is quadratic, it is entirely specified by its Hessian and linear parts:</p>

$$ f_1(x) = \tfrac{1}{2}\langle Qx, x\rangle + \langle q, x\rangle $$

<p>The generic nonlinear term $f_2$ is described by an appropriate function returning its value and gradient
(in this exact order) at any specified point.
The gradient may be computed only when the corresponding output argument is requested.</p>

<p>The nonsmooth term $g$ is described by means of its correspondent proximal mapping, given by</p>

$$\begin{align*}
z &= \mathrm{prox}_{\gamma g}(x) = \mathrm{argmin}_w\left\{ g(w) + \tfrac{1}{2\gamma}\|w-x\|^2\right\} \\
v &= g(z)
\end{align*}$$

<p><strong>Example: sparse logistic regression.</strong> Consider the following problem

$$ \mathrm{minimize}\ \sum_{i=1}^m\log(1+\exp(-b_i \langle a_{i}, x\rangle)) + r\|x\|_1 $$

<p>The smooth term in this case is the logistic function</p>

$$ f_2(x) = \sum_{i=1}^m \log(1+\exp(-x_i)) $$

<p>that can be defined in MATLAB (with its gradient) as follows</p>

<pre><code>function [fz, gradfz] = LogReg(z)
    pz = 1./(1+exp(-z));
    fz = -sum(log(pz));
    if nargout &gt;= 2
        gradfz = (pz-1);
    end
end
</code></pre>

<p>The proximal mapping of the nonsmooth term $g(x) = r\|x\|_1$ (the $L^1$-norm) is given by the soft-thresholding
operator, that can be computed componentwise as</p>

$$ \left[\mathrm{prox}_{\gamma g}(x)\right]_i = \mathrm{sign}(x_i)\cdot\max\{0, |x_i|-\gamma r\}$$

<p>and implemented in MATLAB with the following function</p>

<pre><code>function [z, v] = L1Norm(x, gam, r)
    uz = max(0, abs(x)-gam*r);
    if nargout &gt;= 2
        v = r*sum(uz);
    end
    z = sign(x).*uz;
end
</code></pre>

<p>Therefore, to solve the problem we will need to execute</p>

<pre><code>prob.C = diag(sparse(b))*A;
prob.f2 = @(x) LogReg(x);
prob.g = @(x, gam) L1Norm(x,gam,reg);
prob.x0 = zeros(n,1);
out = minfbe(prob);
</code></pre>

<p>The <code>out</code> structure will contain the results of the optimization process, including the computed solution
and some additional information like the progress of the algorithm during the iterations.</p>

<p>In the following table we summarize all the attributes that may be used to define problem $(1)$.
Notice that <strong>most of them are optional</strong>.</p>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Mandatory?</th>
<th>Default</th>
<th>What is it</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>prob.x0</code></td>
<td>vector</td>
<td>yes</td>
<td>-</td>
<td>The starting point for the algorithm.</td>
</tr>
<tr>
<td>
<code>prob.Q</code> <br> <code>prob.q</code>
</td>
<td>matrix (or function) and vector</td>
<td>no</td>
<td>$Q = 0$ <br> $q = 0$</td>
<td>The Hessian and linear parts of the quadratic term $f_1$.</td>
</tr>
<tr>
<td>
<code>prob.A</code> <br> <code>prob.b</code>
</td>
<td>matrix (or function) and vector</td>
<td>no</td>
<td>$A = I$ <br> $b = 0$</td>
<td>The affine mapping with which $f_1$ is composed.</td>
</tr>
<tr>
<td><code>prob.AT</code></td>
<td>function</td>
<td>yes, if $A$ is defined as a function</td>
<td>-</td>
<td>The function computing the adjoint of $A$.</td>
</tr>
<tr>
<td><code>prob.lin</code></td>
<td>vector</td>
<td>no</td>
<td>0</td>
<td>The linear term $\ell$ in the objective.</td>
</tr>
<tr>
<td><code>prob.f2</code></td>
<td>function</td>
<td>no</td>
<td>the zero function</td>
<td>A procedure returning the value of $f_2$ (1st output) and $\nabla f_2$ (2nd output) at the specified point. It can optionally return $\nabla^2 f_2$ as 3rd ouput.</td>
</tr>
<tr>
<td><code>prob.useHessian</code></td>
<td>boolean, integer</td>
<td>no</td>
<td>0</td>
<td>A flag indicating whether `prob.f2` returns also $\nabla^2 f_2$.</td>
</tr>
<tr>
<td>
<code>prob.C</code> <br> <code>prob.d</code>
</td>
<td>matrix or function, vector</td>
<td>no</td>
<td>$C = I$ <br> $d = 0$</td>
<td>The affine mapping with which $f_2$ is composed.</td>
</tr>
<tr>
<td><code>prob.CT</code></td>
<td>function</td>
<td>yes, if $C$ is defined as a function</td>
<td>-</td>
<td>The function computing the adjoint of $C$.</td>
</tr>
<tr>
<td><code>prob.Lf1</code></td>
<td>real</td>
<td>no</td>
<td>computed numerically</td>
<td>The 2-norm of matrix $A^TQA$.</td>
</tr>
<tr>
<td><code>prob.Lf2</code></td>
<td>real</td>
<td>no</td>
<td>computed numerically</td>
<td>The Lipschitz constant of $\nabla f_2$.</td>
</tr>
<tr>
<td><code>prob.normC</code></td>
<td>real</td>
<td>no</td>
<td>computed numerically</td>
<td>The 2-norm of matrix $C$.</td>
</tr>
<tr>
<td><code>prob.g</code></td>
<td>function</td>
<td>yes</td>
<td>-</td>
<td>A procedure that given $x$ and $\gamma$ (in this order) computes the $\mathrm{prox}_{\gamma g}(x)$ (1st output) and $g(\mathrm{prox}_{\gamma g}(x))$ (2nd output).</td>
</tr>
</tbody>
</table>

<h2 class="inset-text">
<a id="miname-convex-separable-problems" class="anchor" href="#miname-convex-separable-problems" aria-hidden="true"><span class="octicon octicon-link"></span></a>miname (convex separable problems)</h2>

<p>We consider now problems with linear equality constraints, of the following form:</p>

$$\begin{align*}
\mathrm{minimize}\ & f_1(x_1) + f_2(x_2) + g(z) \tag{2}\\
\mathrm{subject\ to}\ & A_1x_1 + A_2x_2 + Bz = c
\end{align*}$$

<p>with $f_1$ (if present) strongly convex and quadratic plus the indicator of an affine subspace, $f_2$ (if present) strongly
convex and twice continuously differentiable in the interior of its domain, while $g$ is proper, closed
and convex. Linear operators $A_1, A_2, B$ need to be specified only if the corresponding term in the 
objective is present, and they are of appropriate dimension along with vector $c$ in the constraints.</p>

<p>The problem is described by specifying the constraint and providing appropriate procedures for computing
the primal iterates (and the corresponding objective values) given a dual variable. Specifically:</p>

$$\begin{align*}
x_1 &= x_1(w) = \mathrm{argmin}_u \left\{f_1(u) - \langle w, u\rangle\right\} \\
x_2 &= x_2(w) = \mathrm{argmin}_u \left\{f_2(u) - \langle w, u\rangle\right\} && \mathrm{and\ } f_2(x_2)\\
z &= z_\gamma(y) = \mathrm{argmin}_v \left\{g(v) + \tfrac{\gamma}{2}\|\gamma^{-1}y+Bv\|^2\right\} && \mathrm{and\ } g(z)
\end{align*}$$

<p><strong>Example: support vector machines.</strong> SVMs can be cast as the following convex optimization problem</p>
$$ \begin{align*}
\mathrm{minimize}\ &\frac{\lambda}{2}\|x\|_2^2 + \sum_{i=1}^m \max\{0, 1-b_iz_i\},\\
\mathrm{subject\ to}\ &Ax = z
\end{align*}$$
<p>Therefore we have in problem $(2)$</p>
$$ x_1 = x,\ f_1(x) = \frac{\lambda}{2}\|x\|_2^2,\ g(z) = \sum_{i=1}^m \max\{0, 1-b_iz_i\},\ A_1 = A,\ B = -I$$
<p>Function $g$ is called <em>hinge loss</em>, and the $x$ and $z$ update steps can be formulated as</p>
$$\begin{align*}
x(w) &= \mathrm{argmin}_u \left\{\tfrac{\lambda}{2}\|u\|_2^2 - \langle w, u\rangle\right\} = \lambda^{-1}w \\
z_{\gamma}(y) &= \mathrm{argmin}_v \left\{g(v) + \tfrac{\gamma}{2}\|\gamma^{-1}y-v\|_2^2\right\} = \mathrm{prox}_{\gamma^{-1} g}(\gamma^{-1}y)
\end{align*}$$
<p>The proximal mapping of $g$ can be computed componentwise, its $i$-th component being</p>
$$\left[\mathrm{prox}_{\gamma g}(x)\right]_i = \begin{cases}b_i\min\{1, b_ix_i+\gamma\} & \mathrm{if}\ b_ix_i < 1 \\ x_i & \mathrm{otherwise}\end{cases}$$
<p>Therefore the problem can be solved with <code>miname</code> with</p>
<pre><code>function [prox, g] = Hinge(x, gam, b)
    bx = b.*x ; ind = bx &lt; 1;
    prox( ind,1) = b( ind).*min(bx(ind)+gam, 1);
    prox(~ind,1) = x(~ind);
    g = sum(max(0, 1-b.*prox));
end
</code></pre>

<pre><code>prob.x1step = @ (w) w/lam ;
prob.zstep = @ (y, gam) Hinge(y/gam, 1/gam, b);
prob.A1 = A;
prob.B = -1;
prob.c = zeros(m, 1);
out = miname(prob);
</code></pre>

<p>The following table summarizes all the attributes that can be used to define problem $(2)$.
Again, notice that <strong>many of them are optional</strong>.</p>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Mandatory?</th>
<th>Default</th>
<th>What is it</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>prob.x1step</code></td>
<td>function</td>
<td>no</td>
<td>-</td>
<td>Procedure computing $x_1(w) = \mathrm{argmin}_u \left\{f_1(u) - \langle w, u\rangle\right\}$, given $w$ (since $f_1$ is quadratic, this procedure computes an affine mapping).</td>
</tr>
<tr>
<td><code>prob.A1</code></td>
<td>matrix or function</td>
<td>yes, if <code>prob.x1step</code> is defined</td>
<td>-</td>
<td>Matrix $A_1$ in the constraint.</td>
</tr>
<tr>
<td><code>prob.A1T</code></td>
<td>function</td>
<td>yes, if <code>prob.A1</code> is defined as a function</td>
<td>-</td>
<td>Procedure computing the adjoint mapping of $A_1$.</td>
</tr>
<tr>
<td><code>prob.x2step</code></td>
<td>function</td>
<td>no</td>
<td>-</td>
<td>Procedure computing $x_2(w) = \mathrm{argmin}_u \left\{f_2(u) - \langle w, u\rangle\right\}$, given <em>w</em>, and $f_2(x_2(w))$.</td>
</tr>
<tr>
<td><code>prob.A2</code></td>
<td>matrix or function</td>
<td>yes, if <code>prob.x2step</code> is defined</td>
<td>-</td>
<td>Matrix $A_2$ in the constraint.</td>
</tr>
<tr>
<td><code>prob.A2T</code></td>
<td>function</td>
<td>yes, if <code>prob.A2</code> is defined as a function</td>
<td>-</td>
<td>Procedure computing the adjoint mapping of $A_2$.</td>
</tr>
<tr>
<td><code>prob.zsetp</code></td>
<td>function</td>
<td>yes</td>
<td>-</td>
<td>Procedure computing $z_\gamma(y) = \mathrm{argmin}_v \left\{g(v) + \tfrac{\gamma}{2}\|\gamma^{-1}y+Bv\|^2\right\}$ and $g(z_\gamma(y))$, given $y$ and $\gamma$.</td>
</tr>
<tr>
<td><code>prob.B</code></td>
<td>matrix</td>
<td>yes</td>
<td>-</td>
<td>Matrix $B$ in the constraint.</td>
</tr>
<tr>
<td><code>prob.c</code></td>
<td>vector</td>
<td>yes</td>
<td>-</td>
<td>The right hand side $c$ of the constraint.</td>
</tr>
</tbody>
</table>

<h2 class="inset-text">
<a id="options" class="anchor" href="#options" aria-hidden="true"><span class="octicon octicon-link"></span></a>Options</h2>

<p>Optional settings may be enabled by specifying the correspondent fields in the <code>opt</code> structure passed
as second argument to <code>minfbe</code> and <code>miname</code>.</p>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Default</th>
<th>What is it</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>opt.tolOpt</code></td>
<td>real</td>
<td>$10^{-5}$</td>
<td>Tolerance on the optimality condition.</td>
</tr>
<tr>
<td><code>opt.maxit</code></td>
<td>integer</td>
<td>$100 n$
</td>
<td>Maximum number of iterations.</td>
</tr>
<tr>
<td><code>opt.method</code></td>
<td>string</td>
<td>'lbfgs'</td>
<td>Algorithm to use for computing descent steps. Can select between: <br> 'sd' (steepest descent) <br> 'lbfgs' (limited memory BFGS) <br>  'cg-desc', 'cg-prp', 'cg-dyhs' (various nonlinear CG algorithms) <br> 'bb' (Barzilai-Borwein).</td>
</tr>
<tr>
<td><code>opt.variant</code></td>
<td>string</td>
<td>'global'</td>
<td>'basic': Use the basic algorithm<br> 'global': Use the <strong>global</strong> variant<br> 'fast': Use the <strong>fast</strong> variant</td>
</tr>
<tr>
<td><code>opt.linesearch</code></td>
<td>string</td>
<td>method dependant</td>
<td>Line search strategy to use. Can select between: <br> 'armijo' (default for 'sd') <br> 'nonmonotone-armijo' (default for 'bb') <br> 'hager-zhang' (default for the rest) <br> 'lemarechal' <br> 'fletcher'</td>
</tr>
</tbody>
</table>

<h2 class="inset-text">
<a id="credits" class="anchor" href="#credits" aria-hidden="true"><span class="octicon octicon-link"></span></a>Credits</h2>

<p>ForBES is developed by Lorenzo Stella [<code>lorenzo.stella-at-imtlucca.it</code>] and Panos Patrinos [<code>panagiotis.patrinos-at-imtlucca.it</code>]
at <a href="http://www.imtlucca.it" target="_blank">IMT Lucca</a>.
Any feedback, bug report or suggestion for future improvements is more than welcome.
We recommend using the <a href="https://github.com/lostella/ForBES/issues" target="_blank">issue tracker</a> on GitHub to report bugs.</p>
      </section>
    </div>
    <footer>
      <p>Project maintained by <a href="https://github.com/lostella">lostella</a></p>
      <p>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></p>
    </footer>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>
